{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umělé neuronové sítě typu MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_loss(iterations, loss, epoch=False):\n",
    "    plt.plot(iterations, loss)\n",
    "    plt.xlabel(\"Epocha\" if epoch else \"Iterace\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Průběh trénování\")\n",
    "    plt.show()\n",
    "\n",
    "npzfile = np.load('data/data_10.npz')\n",
    "npzfile.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = npzfile['x']\n",
    "xTest = npzfile['xTest']\n",
    "\n",
    "y = npzfile['y']\n",
    "yTest = npzfile['yTest']\n",
    "\n",
    "print(f\"{x.shape=}\")\n",
    "print(f\"{y.shape=}\")\n",
    "print(f\"Počet tříd: {np.max(y) + 1}\")\n",
    "\n",
    "w1test = npzfile['w1']\n",
    "w2test = npzfile['w2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stavební bloky sítě"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkce sigmoid\n",
    "\n",
    "$$ sigmoid(u) = \\sigma (u) = \\frac{e^u}{1+e^u} = \\frac{1}{1+e^{-u}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "u = np.array([[1,2],[-3,-4]])\n",
    "sigmoid(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0.73105858, 0.88079708],\n",
    "       [0.04742587, 0.01798621]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivace funkce sigmoid:\n",
    "$$ \\sigma' (u) = \\sigma (u) (1 - \\sigma(u)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(u):\n",
    "    \n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "sigmoid_grad(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0.19661193, 0.10499359],\n",
    "       [0.04517666, 0.01766271]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "$$ f(u) = max(0, u) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "relu(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[1, 2],\n",
    "       [0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivace funkce ReLU:\n",
    "$$ f'(x) = \\boldsymbol{1} (x \\ge 0)$$\n",
    "\n",
    "Derivace přímo v bodě nula je dodefinována na hodnotu nula.\n",
    "\n",
    "Gradient se přes tento blok přenáší:\n",
    "1) Nezměněný, pokud je hodnota na vstupu z dopředného průchodu větší než nula.\n",
    "2) Přenesená hodnota je nula, pokud je hodnota na vstupu z dopředného průchodu menší nebo rovna nule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "relu_grad(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[ True,  True],\n",
    "       [False, False]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "$ \\pi $ nabývá hodnoty 1 pouze pro jednu třídu. Např. máme celkem 3 třídy (0, 1, 2): $\\pi_0 = [0,1,0]$  pro $y_0 = 1$\n",
    "\n",
    "\n",
    "$$\n",
    "    classes = \n",
    "        \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        0 \\\\\n",
    "        2\\\\\n",
    "        1 \\\\\n",
    "        \\end{bmatrix} \n",
    "    \\implies\n",
    "        \\pi = \n",
    "        \\begin{bmatrix}\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        \\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "encoded = one_hot_encoding(y)\n",
    "encoded[[0,900,1800,2700,3500,4200],:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "- Funkce softmax má c vstupů a c výstupů. \n",
    "- Všechny výstupy jsou kladná čísla. \n",
    "- Součet všech výstupů dohromady je roven číslu 1.\n",
    "$$\\widehat{y_c} = softmax(u) = \\frac{e^{u_c}}{\\sum_{d=0}^{c} {e^{u_d}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(u):\n",
    "    \"\"\"\n",
    "    softmax !radkove!\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrola:\n",
    "softmax(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "array([[0.26894142, 0.73105858],\n",
    "       [0.73105858, 0.26894142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_grad(grad_on_output, input_data):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    ...\n",
    "    #################################################################\n",
    "    return weight_grad, bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test vypoctu gradientu pro matici vah a biasy\n",
    "\n",
    "#dva vstupni vektory, kazdy ma 4 hodnoty, cili jde o vrstvu, kde kazdy neuron ma 4 vstupy\n",
    "input_test = np.array([[7,8,4,1],[9,10,4,2]])\n",
    "print(\"input_test\")\n",
    "print(input_test.shape)\n",
    "\n",
    "#dva gradienty na vystupu, kazdy ma 3 hodnoty, cili jde o vrstvu, ktera ma 3 neurony [a kazdy ma 4 vstupy])\n",
    "grad_on_output_test = np.array([[1,2,3],[4,2,6]])\n",
    "print(\"grad_on_output_test\")\n",
    "print(grad_on_output_test.shape)\n",
    "\n",
    "w_grad_test,u_grad_test = theta_grad(grad_on_output_test,input_test)\n",
    "\n",
    "#gradienu vektoru vah ma tedy rozmery 3*4\n",
    "print(\"w_grad_test\")\n",
    "print(w_grad_test.shape)\n",
    "print(w_grad_test)\n",
    "\n",
    "#gradient biasu ma 3 hodnoty\n",
    "print(\"u_grad\")\n",
    "print(u_grad_test.shape)\n",
    "print(u_grad_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "input_test\n",
    "(2, 4)\n",
    "grad_on_output_test\n",
    "(2, 3)\n",
    "w_grad_test\n",
    "(3, 4)\n",
    "[[43 48 20  9]\n",
    " [32 36 16  6]\n",
    " [75 84 36 15]]\n",
    "u_grad\n",
    "(3,)\n",
    "[5 4 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sítě typu vícevrstvý perceptron = Multi-Layer Perceptron (MLP)\n",
    "\n",
    "\n",
    "\n",
    "### Předzpracování dat\n",
    "Pro trénování neuronových sítí je vhodné provádět standardizaci dat na nulovou střední hodnotu a jednotkový rozptyl.\n",
    "\n",
    "### Inicializace parametrů (váhových koeficientů)\n",
    "- Váhy neuronů nesmí být nastaveny na stejné hodnoty (např. 0), aby neměly stejnou hodnotu výstupu a stejný gradient\n",
    "=>\n",
    "- Je třeba porušit symetrii:\n",
    "    - Váhy se inicializují jako malá náhodná čísla (polovina kladná, polovina záporná)\n",
    "    - V praxi se pro ReLU používá hodnota $randn(n) * sqrt(2.0/n)$, kde n je počet vstupů neuronu\n",
    "    - Započítání počtu vstupů pak zajišťuje, že neurony s různým počtem vstupů mají výstup se stejným rozptylem hodnot\n",
    "    - Biasy se inicializují na hodnotu 0 nebo 0.01 (symetrie je již porušena inicializací váhových koeficientů)\n",
    "\n",
    "### Dopředný průchod\n",
    "Kroky:\n",
    "1. $x_1$ je $x$ rozšířená o sloupec bias\n",
    "2. $u_1 = \\theta_1^T x_1$ (vstupní vrstva)\n",
    "3. $a_1 = ReLU(u_1)$ (aktivační funkce, v kódu použijte obecně `activation_function`)\n",
    "4. $x_2$ je $a_1$ rozšířená o sloupec bias\n",
    "5. $u_2 = \\theta_2^T x_2$ (skrytá vrstva)\n",
    "6. $\\tilde{y} = softmax(u_2)$ (výstupní vrstva)\n",
    "\n",
    "Na výstupu vznikne podle zvoleného kritéria chyba či odchylka.\n",
    "\n",
    "### Zpětný průchod\n",
    "\n",
    "(Hodnoty z dopředného průhodu $x$, $a_1$ a $u_2$ je vhodné si z dopředného průchodu uložit.)\n",
    "\n",
    "1. $du_2 = softmax(u_2)-\\pi(y)$\n",
    "\n",
    "2. $dW_2 = du_2 a_1^T$  a  $db_2 = du_2 $\n",
    "\n",
    "3. $da_1 = W_2^T du_2$\n",
    "\n",
    "4. $du_1 = da_1 \\odot relu'(du_1)$ (v kódu použijte obecně `activation_function_derivation`)\n",
    "\n",
    "5. $dW_1 = du_1^T x$  a  $db = du_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron:\n",
    "    def __init__ (self, *, input_layer_size, hidden_layer_size, output_size, activation_function, activation_function_derivation):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        self.w1 = ...\n",
    "        self.w2 = ...\n",
    "\n",
    "        ...\n",
    "        #################################################################\n",
    "        \n",
    "    def forward(self, x) -> tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        Spočítá predikci na základě aktuálních vah.\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "\n",
    "        #1. vrstva\n",
    "        x1 = ... #nezapomente na bias(prvni sloupec) #4500x401\n",
    "        u1 = ...\n",
    "\n",
    "        #aktivacni funkce pomocí funkce self.activation_function\n",
    "        a1 = ... #4500x25\n",
    "\n",
    "        #2. vrstva (skryta vrstva)\n",
    "        x2 = ... #4500x26\n",
    "        u2 = ...\n",
    "\n",
    "        #vystup po softmaxu\n",
    "        scores = ... #4500x10 scores pro kazdou tridu            \n",
    "        \n",
    "        #cache\n",
    "        forward_cache = ...\n",
    "        #################################################################\n",
    "        \n",
    "        # forward_cache je dictionary obsahující uložené hodnoty potřebné pro zpětný průchod\n",
    "        return scores, forward_cache\n",
    "    \n",
    "    def backward(self, classes, forward_cache: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Vypočítá gradienty na základě hodnot uložených při dopředném průchodu.\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "\n",
    "        #pomocí self.scores, self.classes a one_hot_encoding \n",
    "        du2 = ...\n",
    "        \n",
    "        #pomocí funkce theta_grad\n",
    "        dw2, db2 = ...\n",
    "        \n",
    "        da1 = ...\n",
    "        \n",
    "        #pomocí funkce self.activation_function_derivation\n",
    "        #POZOR: tato funce se musí aplikovat na vstupní hodnotu z dopředného průchodu !!\n",
    "        du1 = ...\n",
    "        \n",
    "        #pomocí funkce theta_grad\n",
    "        dw1, db1 = ...\n",
    "        \n",
    "        gradients = ...\n",
    "        #################################################################\n",
    "\n",
    "        # gradients je dictionary obsahující vypočtené gradienty\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def update_weights_gd(self, gradients: dict, alpha, lmbd=0):\n",
    "        \"\"\"\n",
    "        Aktualizuje váhy sítě na základě vypočtených gradientů.\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "    \n",
    "        #POZOR: dw2 není gradient celé matice w2 ale pouze její části\n",
    "        #gradient celé matice w2 pro update vah vznikne vhodným spojením dw2 a db2\n",
    "        ...\n",
    "        \n",
    "        #################################################################\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, classes):\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        ...\n",
    "        #################################################################\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "output_size = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance pro odladění:\n",
    "testTlp = TwoLayerPerceptron(input_layer_size=input_layer_size, hidden_layer_size=hidden_layer_size, output_size=output_size, activation_function=sigmoid, activation_function_derivation=sigmoid_grad)\n",
    "testTlp.w1 = w1test\n",
    "testTlp.w2 = w2test\n",
    "# alpha = 0.0005, lmbd=0\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pro odladění s předchozí instancí po jednom kroku trénování:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "x1[0, 0:4]=array([1., 0., 0., 0.])\n",
    "u1[0, 0:4]=array([ 4.49064643, -9.04210233,  2.84027269,  4.37206593])\n",
    "a1[0, 0:4]=array([9.88910953e-01, 1.18307825e-04, 9.44813682e-01, 9.87532276e-01])\n",
    "x2[0, 0:4]=array([1.00000000e+00, 9.88910953e-01, 1.18307825e-04, 9.44813682e-01])\n",
    "u2[0, 0:4]=array([ -9.3676932 ,  -6.38107877, -12.6179903 ,   5.60001973])\n",
    "scores[0, 0:4]=array([3.15901233e-07, 6.26067973e-06, 1.22451815e-08, 9.99876718e-01])\n",
    "du2[0, 0:4]=array([ 3.15901233e-07,  6.26067973e-06,  1.22451815e-08, -1.23281726e-04])\n",
    "dw2[0, 0:4]=array([4.22991785, 1.02150407, 2.85666977, 3.43761753])\n",
    "db2[0:4]=array([ 8.88586683, -1.67675408, -3.85991324, -0.11891478])\n",
    "da1[0, 0:4]=array([-1.62692031e-05, -1.70178022e-04,  1.70624671e-05, -3.48415311e-04])\n",
    "du1[0, 0:4]=array([-1.78409383e-07, -2.01310098e-08,  8.89650482e-07, -4.28978692e-06])\n",
    "dw1[0, 0:4]=array([ 0.00000000e+00,  0.00000000e+00,  1.33038074e-06, -3.59469771e-05])\n",
    "db1[0:4]=array([-0.28923762,  1.4078135 , -1.52360542, -1.94639874])\n",
    "w1[0, 0:4]=array([-2.24177711e-02, -1.05624163e-08,  2.19414684e-09, -6.92289878e-06])\n",
    "w2[0, 0:4]=array([-0.76544645, -1.21455994, -0.10238206, -2.36992919])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gd(model, x, classes, nIter, alpha=0.00015, lmbd=0):\n",
    "    \"\"\"\n",
    "    Natrénuje síť a vykreslí graf vývoje lossu.\n",
    "    \"\"\"\n",
    "\n",
    "    # Na konci každé iterace vypočtěte cross-entropy loss a ulože ho na odpovídající index v poli.\n",
    "    loss = np.zeros(nIter)\n",
    "\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT  \n",
    "    ...\n",
    "    #################################################################\n",
    "\n",
    "    show_loss(np.arange(nIter), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénování modelu s aktivační funkcí sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlp = TwoLayerPerceptron(input_layer_size=input_layer_size, hidden_layer_size=hidden_layer_size, output_size=output_size, activation_function=sigmoid, activation_function_derivation=sigmoid_grad)\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "...\n",
    "#################################################################\n",
    "print(f\"sigmoid testovaci mnozina : {accuracy}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénování modelu s aktivační funkcí ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlp = TwoLayerPerceptron(input_layer_size=input_layer_size, hidden_layer_size=hidden_layer_size, output_size=output_size, activation_function=relu, activation_function_derivation=relu_grad)\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "...\n",
    "#################################################################\n",
    "print(f\"relu testovaci mnozina : {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: PyTorch a MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: Číslovky z datasetu MNIST z bonusové části 7. cvičení (tam si je můžete prohlédnout).\n",
    "\n",
    "**Dosáhněte accuracy > 97 %**\n",
    "\n",
    "Tip: Menší batche napomáhají generalizaci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('data/data_07_mnist_train.npz') \n",
    "\n",
    "data = npzfile['data']\n",
    "ref = npzfile['ref']\n",
    "\n",
    "# Převod na objekty knihovny PyTorch\n",
    "data = torch.Tensor(data)\n",
    "ref = torch.Tensor(ref).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('data/data_07_mnist_test.npz') \n",
    "\n",
    "test_data = npzfile['data']\n",
    "test_ref = npzfile['ref']\n",
    "\n",
    "# Převod na objekty knihovny PyTorch\n",
    "test_data = torch.Tensor(test_data)\n",
    "test_ref = torch.Tensor(test_ref).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizace hodnot pixelů (na základě statistických odhadů z trénovacích dat)\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "data = (data - mean) / std\n",
    "test_data = (test_data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definice modelu\n",
    "\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "input_layer_size = ... \n",
    "hidden_layer_size = ...\n",
    "output_size = ...\n",
    "#################################################################\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_layer_size, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, output_size),\n",
    "    # SOFTMAX se nepřidává! Kriteriální funkce ho počítá sama. Výstupem sítě jsou tzv. logity.\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Konfigurace hyperparametrů trénování.\n",
    "# Optimizér provádí zvolenou metodu optimalizace sítě (SGD / SGD + momentum / ADAM / jiné).\n",
    "# Parametr weight_decay je lmbd z našeho kódu.\n",
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "num_epochs = ...\n",
    "batch_size = ...\n",
    "optimizer = optim.SGD(model.parameters(), lr=..., weight_decay=..., momentum=...)\n",
    "# optimizer = optim.Adam(...)\n",
    "# optimizer = optim.AdamW(...)\n",
    "#################################################################\n",
    "\n",
    "# Kriteriální funkce: počítá softmax a cross-entropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Nastaví model do trénovacího módu (důležité pro některé typy vrstev, např. Dropout nebo BatchNorm).\n",
    "model.train()\n",
    "\n",
    "losses = np.zeros(num_epochs)\n",
    "num_batches = data.size(0) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Zamíchání dat\n",
    "    permutation = torch.randperm(data.size(0))\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Smyčka přes minibatche\n",
    "    for i in range(0, data.size(0), batch_size):\n",
    "        indices = permutation[i : i + batch_size]\n",
    "        batch_x, batch_y = data[indices], ref[indices]\n",
    "        \n",
    "        # Gradienty jsou uloženy u vah, kterých se týkají. Je potřeba je v každé iteraci explicitně vynulovat, jinak se akumulují.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ekvivalent naší funkce forward (bez softmax).\n",
    "        logits = model(batch_x)\n",
    "        \n",
    "        # Výpočet průměrného lossu podle zvolené kriteriální funkce.\n",
    "        loss = criterion(logits, batch_y)\n",
    "        \n",
    "        # Spočítá gradienty. Ty nejsou výstupem funkce, ale jsou uloženy u vah, kterých se týkají.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Aktualizuje váhy podle zvolené metody.\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Průměrný loss.\n",
    "    losses[epoch] = epoch_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {losses[epoch]:.4f}\")\n",
    "\n",
    "show_loss(np.arange(num_epochs), losses, epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nastaví model do testovacího módu (důležité pro některé typy vrstev, např. Dropout nebo BatchNorm).\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad() vypne operace pro výpočet gradientu, které se provádí na pozadí, protože je zde nepotřebujeme.\n",
    "with torch.no_grad():\n",
    "    logits = model(test_data)\n",
    "    predicted_classes = logits.argmax(1)\n",
    "    acc = (predicted_classes == test_ref).float().mean()\n",
    "\n",
    "print(f\"Accuracy: {acc * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2509_vsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
